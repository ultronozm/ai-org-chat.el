#+title: ai-org-chat.el: Threaded chat with AI agent
#+author: Paul Nelson

* Overview
This package supports a threaded AI chat inside any org-mode buffer.  Here's what it looks like:

#+attr_html: :width 800px
#+attr_latex: :width 800px
[[./img/fruits.png]]

The design is inspired by the ChatGPT web interface.  There are [[https://www.reddit.com/r/emacs/comments/14glmqc/use_emacs_as_a_chatgpt_app/][many AI chat packages]] for Emacs, but I am not aware of any that naturally support multiple conversational pathways.

The package comes with one main function, =ai-org-chat-respond=, that operates in any org-mode buffer.  It extracts a conversation history from the parent entries, treating an entry as belonging to the AI if its heading is "AI" and otherwise to the user.  This conversation history is passed along to the OpenAI library to generate a response.

The "plumbing" concerning API calls is outsourced by default to the [[https://github.com/karthink/gptel][gptel]] library, which requires the user to provide an OpenAI API key.  By customizing the variable =ai-org-chat-request-fn=, any other backend could be used.

[[https://www.gnu.org/software/emacs/manual/html_node/emacs/Narrowing.html][Narrowing]] provides a simple way to truncate the conversation history if it becomes too long -- only the restricted part of the buffer is considered.

A "system message" (at least for OpenAI/Anthropic models; I haven't checked more generally) can be specified via the customization variable =ai-org-chat-system-message=.  "Context" can be passed along with the system message to share the contents of other Emacs buffers with the AI (see [[Context]]).

* Configuration
This package depends (by default) upon [[https://github.com/karthink/gptel][gptel]], so you should first set that up and configure it.  The following configuration works for me on MacOS after setting the environment variable "OPENAI_API_KEY" to my API key.

#+begin_src elisp
(use-package exec-path-from-shell
  :ensure
  :init
  (exec-path-from-shell-initialize))
  
(use-package gptel
  :ensure
  :after exec-path-from-shell
  :config
  (setq gptel-api-key (exec-path-from-shell-getenv "OPENAI_API_KEY")))
#+end_src

Next, download this repository, install using =M-x package-install-file= (or package-vc-install, straight, elpaca, ...), and add =(use-package ai-org-chat)= to your [[https://www.emacswiki.org/emacs/InitFile][init file]].  For a slightly more elaborate configuration, use something like the following:

#+begin_src elisp
(use-package ai-org-chat
  :bind
  (:map global-map
        ("C-c /" . ai-org-chat-new))
  (:map ai-org-chat-minor-mode
        ("C-c <return>" . ai-org-chat-respond)
        ("C-c n" . ai-org-chat-branch)
        ("C-c c" . ai-org-chat-set-context))
  :commands (ai-org-chat-minor-mode)
  :custom
  (ai-org-chat-user-name "Paul")
  (ai-org-chat-dir "~/gpt"))
#+end_src

* Usage
When you want to ask the AI something, do =M-x ai-org-chat-new= (or =C-c /=, if you followed the above configuration).  This visits a new file in the specified directory ("~/gpt" by default).  If the region was active, then it will be quoted in the new buffer.  Example:

#+attr_html: :width 800px
#+attr_latex: :width 800px
[[./img/animated.gif]]

The org-mode buffer has =ai-org-chat-minor-mode= activated, whose only purpose is to support user-defined keybindings like in the above =use-package= declaration.  There are three useful commands:

- =ai-org-chat-respond= (=C-c <return>=) :: This is the main function, which tells the AI to generate a new response to the conversation node at point.  It works in /any/ org-mode buffer, not just ones created via =ai-org-chat-new=.
- =ai-org-chat-branch= (=C-c n=) :: This is a convenience function that creates a new conversation branch at point.
- =ai-org-chat-set-context= (=C-c c=) :: This sets =ai-org-chat-context= (see below).

* Context
The buffer-local customization variable =ai-org-chat-context= can currently attain three values: nil, =visible-contents= and =visible-buffers=.  In the latter two cases, the system message is appended with some of the contents of the visible buffers.
- If =visible-contents= is selected, then we append the visible content of the visible buffers.
- If =visible-buffers= is selected, then we append the complete content of the visible buffers.

This makes the chat client behave a bit like a pair-programmer.

This feature combines well with narrowing and ediff (TODO: document better, maybe with a video).
